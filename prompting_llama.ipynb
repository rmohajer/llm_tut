{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This code is to try and learn prompt engineering using Llama 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "import ollama\n",
    "from langchain_community.llms import Ollama\n",
    "from crewai import Agent, Task, Crew, Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Ollama(model=\"llama3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic prompt enegineering techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "email = ''' \n",
    "subject: Urgent, please sign the approval\n",
    "Text: dear Reza,\n",
    "        Hope this email finds you well, we need your signature to proceed with the deployment of the PFR model.\n",
    "    please reach out if any questions.\n",
    "    Sincerely,\n",
    "    AI platform team\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_content = ''' \n",
    "    You are an AI assistant whoes job is to accurately classify emails in an honest and upfront way. \n",
    "    there are only 3 categories for importance rating: 'important', 'casual', 'spam'.\n",
    "    You need to be careful not to let any spam email to pass through, so should be inclined to rating suspicious emails as spam. \n",
    "    reply with the final category and the reason for that.\n",
    "    '''\n",
    "user_content = f\"what is the category of this email: {email}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ollama.chat(model='llama3', \n",
    "                       messages=[\n",
    "                           {'role': 'system', 'content': system_content},\n",
    "                           {'role': 'user', 'content': user_content}\n",
    "                                 ],\n",
    "                        stream=False,\n",
    "                        options = {'temperature' : 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I would categorize this email as \"important\".\n",
      "\n",
      "The reason for this is that the subject line indicates a sense of urgency and importance, and the text mentions a specific action (signing an approval) that needs to be taken. The tone of the email is professional and polite, which suggests that it's not spam or casual in nature. Additionally, the email appears to be from a legitimate team (AI platform team), which further supports its categorization as important.\n"
     ]
    }
   ],
   "source": [
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'llama3',\n",
       " 'created_at': '2024-04-25T08:36:14.756955Z',\n",
       " 'message': {'role': 'assistant',\n",
       "  'content': 'I would categorize this email as \"important\".\\n\\nThe reason for this is that the subject line indicates a sense of urgency and importance, and the text mentions a specific action (signing an approval) that needs to be taken. The tone of the email is professional and polite, which suggests that it\\'s not spam or casual in nature. Additionally, the email appears to be from a legitimate team (AI platform team), which further supports its categorization as important.'},\n",
       " 'done': True,\n",
       " 'total_duration': 4605200875,\n",
       " 'load_duration': 2931417,\n",
       " 'prompt_eval_duration': 1043773000,\n",
       " 'eval_count': 94,\n",
       " 'eval_duration': 3551423000}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I would categorize this email as \"important\".\n",
      "\n",
      "The reason for this is that the subject line indicates a sense of urgency and importance, and the text mentions a specific action (signing an approval) that needs to be taken. The tone of the email is professional and polite, which suggests that it's not spam or casual in nature. Additionally, the email appears to be from a legitimate team (AI platform team), which further supports its categorization as important."
     ]
    }
   ],
   "source": [
    "# alternative way to print the outputs\n",
    "stream = ollama.chat(model='llama3', \n",
    "                       messages=[\n",
    "                           {'role': 'system', 'content': system_content},\n",
    "                           {'role': 'user', 'content': user_content}\n",
    "                                 ],\n",
    "                        stream=True,\n",
    "                        options = {'temperature' : 0})\n",
    "\n",
    "assistant_message = ''\n",
    "for chunk in stream:\n",
    "    assistant_message += chunk['message']['content']\n",
    "    print(chunk['message']['content'], end='', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing an empty list for storing the chat messages and setting up the initial system message\n",
    "chat_messages = []\n",
    "system_message='You are a helpful assistant.'\n",
    "\n",
    "# Defining a function to create new messages with specified roles ('user' or 'assistant')\n",
    "def create_message(message, role):\n",
    "  return {\n",
    "    'role': role,\n",
    "    'content': message\n",
    "  }\n",
    "\n",
    "# Starting the main conversation loop\n",
    "def chat():\n",
    "  # Calling the ollama API to get the assistant response\n",
    "  ollama_response = ollama.chat(model='llama3', stream=True, messages=chat_messages)\n",
    "\n",
    "  # Preparing the assistant message by concatenating all received chunks from the API\n",
    "  assistant_message = ''\n",
    "  for chunk in ollama_response:\n",
    "    assistant_message += chunk['message']['content']\n",
    "    print(chunk['message']['content'], end='', flush=True)\n",
    "    \n",
    "  # Adding the finalized assistant message to the chat log\n",
    "  chat_messages.append(create_message(assistant_message, 'assistant'))\n",
    "\n",
    "# Function for asking questions - appending user messages to the chat logs before starting the `chat()` function\n",
    "def ask(message):\n",
    "  chat_messages.append(\n",
    "    create_message(message, 'user')\n",
    "  )\n",
    "  print(f'\\n\\n--{message}--\\n\\n')\n",
    "  chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--Please list the 20 largest cities in the world.--\n",
      "\n",
      "\n",
      "Here are the 20 largest cities in the world, ranked by population:\n",
      "\n",
      "1. **Tokyo, Japan**: 38,140,000\n",
      "2. **Delhi, India**: 29,280,000\n",
      "3. **Shanghai, China**: 24,160,000\n",
      "4. **Mumbai, India**: 21,380,000\n",
      "5. **SÃ£o Paulo, Brazil**: 21,310,000\n",
      "6. **Mexico City, Mexico**: 20,900,000\n",
      "7. **Osaka, Japan**: 20,310,000\n",
      "8. **Kolkata, India**: 18,740,000\n",
      "9. **Dhaka, Bangladesh**: 18,540,000\n",
      "10. **Beijing, China**: 17,520,000\n",
      "11. **Lagos, Nigeria**: 16,770,000\n",
      "12. **Buenos Aires, Argentina**: 16,430,000\n",
      "13. **Cairo, Egypt**: 15,850,000\n",
      "14. **Los Angeles, USA**: 15,620,000\n",
      "15. **Calcutta, India**: 15,510,000\n",
      "16. **Chicago, USA**: 15,440,000\n",
      "17. **Bangkok, Thailand**: 14,920,000\n",
      "18. **Nairobi, Kenya**: 14,550,000\n",
      "19. **Istanbul, Turkey**: 14,420,000\n",
      "20. **Kuala Lumpur, Malaysia**: 14,330,000\n",
      "\n",
      "Note: Population figures are for the urban agglomeration (city proper) and are based on latest available estimates from reliable sources such as city governments, national statistics agencies, or international organizations like the United Nations.\n",
      "\n",
      "Keep in mind that these numbers are approximate and may have changed slightly since the last official census or estimation. Additionally, the definition of a \"city\" can be subjective, and different sources may use different criteria to determine what constitutes a city's boundaries."
     ]
    }
   ],
   "source": [
    "ask('Please list the 20 largest cities in the world.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Email Classification using agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Agent(\n",
    "    role = \"email classifier\",\n",
    "    goal = \"accurately classify emails based on their importance into one of the following 3 categories: important, casual, spam\",\n",
    "    backstory = \"You are an AI assistant whoes job is to accurately classify emails in an honest and upfront way. \\\n",
    "        You need to be careful not to let any spam email to pass through, so should be inclined to rating suspicious emails as spam.\",\n",
    "    allow_delegation =  False,\n",
    "    verbose = False,\n",
    "    llm = model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "responder = Agent(\n",
    "    role = \"email responder\",\n",
    "    goal = \"Based on the importance rating of the email write a concise response. If the email is classified as spam, just ignore it.\\\n",
    "        if the email is classified as important, write a short formal response. \\\n",
    "        and if the email is classified as casual, just write a friendly response back\",\n",
    "    backstory = \"You are an AI assistnat whoes job is to provide a short response to an email based on the importance rating.\\\n",
    "    The importance rating of the email is classified by the 'classifier' agent.\",\n",
    "    allow_delegation =  False,\n",
    "    verbose  = False,\n",
    "    llm = model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-25 18:28:25,938 - 7921688256 - __init__.py-__init__:518 - WARNING: Overriding of current TracerProvider is not allowed\n"
     ]
    }
   ],
   "source": [
    "email_classfier = Task(\n",
    "    description = f\"classify the following email: {email}\",\n",
    "    agent = classifier,\n",
    "    expected_output = \"One of the following options: 'important', 'casual', 'spam'.\"\n",
    ")\n",
    "\n",
    "email_responder = Task(\n",
    "    description = f\"Write a response to the following email: {email}, based on the importance rating identified by the 'responder' agent.\",\n",
    "    agent = responder,\n",
    "    expected_output = \"A concise response based on the importance rating identified by the 'responder' agent.\"\n",
    ")\n",
    "\n",
    "crew = Crew(\n",
    "    agents = [classifier],\n",
    "    tasks = [email_classfier],\n",
    "    process = Process.sequential,\n",
    "    verbose = 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m [DEBUG]: == Working Agent: email classifier\u001b[00m\n",
      "\u001b[1m\u001b[95m [INFO]: == Starting Task: classify the following email:  subject: Urgent, please sign the approval\n",
      "        Text: dear Reza,\n",
      "        Hope this email finds you well, we need your signature to proceed with the deployment of the PFR model.\n",
      "    please reach out if any questions.\n",
      "    Sincerely,\n",
      "    AI platform team\n",
      "    \u001b[00m\n",
      "\u001b[1m\u001b[92m [DEBUG]: == [email classifier] Task output: 'spam''<|eot_id|>assistant<|eot_id|><|eot_id|><|eot_id|><|eot_id|> <|eot_id|> <|eot_id|> <|eot_id|> <|eot_id|> <|eot_id|> <|eot_id|> <|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|start_header_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|>\n",
      "\n",
      "\u001b[00m\n",
      "'spam''<|eot_id|>assistant<|eot_id|><|eot_id|><|eot_id|><|eot_id|> <|eot_id|> <|eot_id|> <|eot_id|> <|eot_id|> <|eot_id|> <|eot_id|> <|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|start_header_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "output = crew.kickoff()\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'spam''<|eot_id|>assistant<|eot_id|><|eot_id|><|eot_id|><|eot_id|> <|eot_id|> <|eot_id|> <|eot_id|> <|eot_id|> <|eot_id|> <|eot_id|> <|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|start_header_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "\n",
    "- You can see that this process takes a long time to complete\n",
    "- Looks like direct usage of ollama in terminal could be faster\n",
    "- You can use [Groq](https://wow.groq.com/) to accelerate the process significantly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_tut",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
